{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25de4a68",
   "metadata": {},
   "source": [
    "## TEXT PRE-PROCESSING USING SPACY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93a031",
   "metadata": {},
   "source": [
    "### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44e476f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy                                                 \n",
    "from spacy.lang.en import English \n",
    "nlp = spacy.load(\"en_core_web_sm\")  \n",
    "from spacy.lang.en.stop_words import STOP_WORDS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce66e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 326\n",
      "First ten stop words: ['but', \"'s\", '‘ll', 'nor', 'often', \"n't\", 'through', 'ourselves', 'upon', 'everyone', 'us', 'themselves', 'say', 'whither', 'whereafter']\n"
     ]
    }
   ],
   "source": [
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS \n",
    "print('Number of stop words: %d' % len(spacy_stopwords)) \n",
    "print('First ten stop words: %s' % list(spacy_stopwords)[:15]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2276e0",
   "metadata": {},
   "source": [
    "### TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5bc1fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens=['cries','this','lied','computing','organizing','matches']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "469b1d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries --> cry\n",
      "this --> this\n",
      "lied --> lie\n",
      "computing --> computing\n",
      "organizing --> organizing\n",
      "matches --> match\n"
     ]
    }
   ],
   "source": [
    "#lemmatization\n",
    "doc = nlp(\"cries this lied computing organizing matches\")                                                                                          \n",
    "for word in doc:                                                                            \n",
    "    print(word.text,'-->',word.lemma_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e737bf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cries --> cri\n",
      "this --> this\n",
      "lied --> lie\n",
      "computing --> comput\n",
      "organizing --> organ\n",
      "matches --> match\n"
     ]
    }
   ],
   "source": [
    "#stemming\n",
    "import nltk                                                                                 \n",
    "from nltk.stem.snowball import SnowballStemmer                                              \n",
    "stemmer = SnowballStemmer(language='english')\n",
    "for token in tokens:                                                                     \n",
    "    print(token + ' --> ' + stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6f20190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After stemming: cries --> cri\n",
      "After stemming: this --> this\n",
      "After stemming: lied --> lie\n",
      "After stemming: computing --> comput\n",
      "After stemming: organizing --> organ\n",
      "After stemming: matches --> match\n",
      "\n",
      "\n",
      "After lemmatization: cries --> cry\n",
      "After lemmatization: this --> this\n",
      "After lemmatization: lied --> lie\n",
      "After lemmatization: computing --> computing\n",
      "After lemmatization: organizing --> organizing\n",
      "After lemmatization: matches --> match\n"
     ]
    }
   ],
   "source": [
    "#comparing stemming and lemmatization\n",
    "for token in tokens:                                                                        \n",
    "    print('After stemming:',token + ' --> ' + stemmer.stem(token))                                  \n",
    "print('\\n')                                                                                  \n",
    "for word in doc:                                                                             \n",
    "    print('After lemmatization:',word.text,'-->',word.lemma_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec483fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization gives better output as a root word while comparing with Stemming\n"
     ]
    }
   ],
   "source": [
    "print('Lemmatization gives better output as a root word while comparing with Stemming')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc91106",
   "metadata": {},
   "source": [
    "### TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43206e05",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note from poster to Kubrick newsgroup:\n",
      "\n",
      "I found this on a bbs a while ago and I thought I'd pass it along to all \n",
      "of you Kubrick freaks out there.\n",
      "\n",
      "02/23/89\n",
      "Transcriber's note:\n",
      "\n",
      "For all you Clarke/Kubrick/2001 fans,\n",
      "\n",
      "I found the original paper copy of this screenplay a while back and felt \n",
      "compelled to transcribe it to disk and upload it to various bulletin \n",
      "boards for the enjoyment of all.\n",
      "\n",
      "The final movie deviates from this screenplay in a number of interesting \n",
      "ways. I've tried to maintain the format of the original document except \n",
      "the number of lines per page of the original. In order to reduce the \n",
      "length of this file I've used a bar of \"------\" to delimit the pages as \n",
      "there was a lot of whitespace per original screenplay page.\n"
     ]
    }
   ],
   "source": [
    "#A)scifiscripts_intro.txt\n",
    "data=open('scifiscripts_intro.txt').read()\n",
    "data1=nlp(data)\n",
    "print(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8b2397c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removal of stop words: [Note, poster, Kubrick, newsgroup, :, \n",
      "\n",
      ", found, bbs, ago, thought, pass, \n",
      ", Kubrick, freaks, ., \n",
      "\n",
      ", 02/23/89, \n",
      ", Transcriber, note, :, \n",
      "\n",
      ", Clarke, /, Kubrick/2001, fans, ,, \n",
      "\n",
      ", found, original, paper, copy, screenplay, felt, \n",
      ", compelled, transcribe, disk, upload, bulletin, \n",
      ", boards, enjoyment, ., \n",
      "\n",
      ", final, movie, deviates, screenplay, number, interesting, \n",
      ", ways, ., tried, maintain, format, original, document, \n",
      ", number, lines, page, original, ., order, reduce, \n",
      ", length, file, bar, \", ------, \", delimit, pages, \n",
      ", lot, whitespace, original, screenplay, page, .]\n"
     ]
    }
   ],
   "source": [
    "#removal of stop words for (A)\n",
    "filtered_data=[]                                                                                                                     \n",
    "for word in data1:                                                         \n",
    "    if word.is_stop==False:                                               \n",
    "        filtered_data.append(word)                                        \n",
    "print(\"Sentence after removal of stop words:\",filtered_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d829a8e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the first sentence.And followed by first, is the second sentence.With the help of second, we get third sentence.After succeeded by third sentence, we get fourth sentence.Finally, we get fifth sentence.\n"
     ]
    }
   ],
   "source": [
    "#B)Five sentence by own\n",
    "d=\"This is the first sentence.And followed by first, is the second sentence.With the help of second, we get third sentence.After succeeded by third sentence, we get fourth sentence.Finally, we get fifth sentence.\"\n",
    "d1=nlp(d)\n",
    "print(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da154c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removal of stop words: [sentence, ., followed, ,, second, sentence, ., help, second, ,, sentence, ., succeeded, sentence, ,, fourth, sentence, ., Finally, ,, fifth, sentence, .]\n"
     ]
    }
   ],
   "source": [
    "#removal of stop words for (B)\n",
    "filtered_data1=[]                                                                                                                     \n",
    "for word in d1:                                                         \n",
    "    if word.is_stop==False:                                               \n",
    "        filtered_data1.append(word)                                        \n",
    "print(\"Sentence after removal of stop words:\",filtered_data1)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86da333",
   "metadata": {},
   "source": [
    "### TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ed65fc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bryan \t PROPN \t NNP\n",
      "visited \t VERB \t VBD\n",
      "his \t PRON \t PRP$\n",
      "friend \t NOUN \t NN\n",
      "for \t ADP \t IN\n",
      "a \t DET \t DT\n",
      "while \t NOUN \t NN\n",
      "and \t CCONJ \t CC\n",
      "then \t ADV \t RB\n",
      "went \t VERB \t VBD\n",
      "home \t ADV \t RB\n",
      "at \t ADP \t IN\n",
      "10 \t NUM \t CD\n",
      "pm \t NOUN \t NN\n",
      ". \t PUNCT \t .\n"
     ]
    }
   ],
   "source": [
    "#pos and tag with description\n",
    "Text=nlp('Bryan visited his friend for a while and then went home at 10 pm.')\n",
    "for word in Text:                                       \n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf252c1",
   "metadata": {},
   "source": [
    "### TASK 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbd82016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#find proper nouns and numbers from the file\n",
    "file=open('Random.txt').read()\n",
    "File=nlp(file)\n",
    "print(File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4cc765b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "#pos\n",
    "noun=[]\n",
    "num=[]\n",
    "for word in File:                                       \n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)\n",
    "    if(word.pos_=='PROPN'):\n",
    "        noun.append(word.text)\n",
    "    elif(word.pos_=='NUM'):\n",
    "        num.append(word.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a4c748f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proper nouns are: ['PADUA', 'HIGH', 'SCHOOL', 'DAY', 'Revision', 'November']\n",
      "The numbers are: ['12', '1997']\n"
     ]
    }
   ],
   "source": [
    "print('The proper nouns are:',noun)\n",
    "print('The numbers are:',num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89c9361",
   "metadata": {},
   "source": [
    "### TASK 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da87df45",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', \"'s\", '‘ll', 'nor', 'often', \"n't\", 'through', 'ourselves', 'upon', 'everyone', 'us', 'themselves', 'say', 'whither', 'whereafter', 'former', 'n‘t', 'nothing', 'have', 'give', 'whoever', 'has', 'under', 'except', \"'d\", 'regarding', 'whose', 'hereby', 'full', 'least', 'take', 'beyond', 'thru', 'made', 'at', 'everything', 'became', 'whereupon', 'move', 'did', 'whatever', 'else', 'own', 'any', 'used', '‘re', 'although', 'formerly', 'wherein', 'next', 'name', 'down', 'five', 'out', 'using', 'would', 'who', \"'re\", 'elsewhere', 'before', 'last', 'that', 'top', 'twelve', 'until', 'seems', 'eight', 'alone', 'together', 'not', 'therefore', 'now', 'since', 'otherwise', 'less', 'put', 'all', 'seemed', 'n’t', 'your', 'most', 'they', 'amongst', 'few', 'Word5', 'perhaps', 'off', 'she', 'whereas', 'noone', 'hers', 'anyone', 'such', 'yourself', 'really', 'get', 'mine', 'part', 'many', \"'m\", 'well', 'our', 're', 'side', 'several', 'six', 'Word1', 'had', 'yourselves', 'ours', '‘m', 'being', 'yours', 'empty', 'becomes', 'what', 'among', 'the', 'done', 'nine', 'doing', 'toward', 'first', 'further', 'was', 'enough', 'much', 'thereby', 'latter', 'another', 'seeming', 'Word2', 'almost', 'be', 'into', 'please', 'serious', 'thereafter', 'become', 'below', 'this', 'hence', 'might', 'bottom', '‘d', 'just', 'both', '‘s', 'anything', 'sixty', 'to', 'cannot', 'too', \"'ll\", 'besides', 'somehow', 'we', 'each', 'by', 'or', 'ten', 'do', 'so', 'forty', 'sometime', 'towards', 'my', 'three', 'same', 'see', 'because', 'front', 'across', 'fifteen', 'call', 'herein', 'if', 'sometimes', 'nowhere', 'wherever', 'thus', '’re', 'third', 'indeed', 'seem', 'thereupon', 'whom', 'ca', 'in', 'throughout', 'other', '‘ve', 'over', 'is', 'ever', 'while', 'himself', 'anyhow', 'then', 'were', 'per', 'when', 'however', 'here', 'of', 'their', 'some', 'does', 'with', 'latterly', 'also', 'he', 'no', 'various', 'mostly', 'make', 'moreover', '’ll', 'quite', 'been', 'keep', 'above', 'about', 'on', 'nevertheless', 'whole', 'namely', 'there', 'whenever', 'an', 'herself', 'once', 'something', 'anywhere', 'rather', 'him', 'whence', 'a', 'though', 'during', 'along', 'hundred', 'its', 'may', 'you', 'onto', 'either', 'very', 'hereafter', 'those', 'none', 'via', '’d', 'and', 'beside', 'how', 'yet', 'where', 'it', 'am', 'nobody', '’s', 'can', 'between', 'around', 'as', 'Word3', 'i', 'anyway', 'Word4', 'beforehand', 'never', 'whereby', 'already', 'every', 'show', 'me', 'could', 'thence', 'afterwards', 'these', 'due', 'somewhere', 'two', 'more', 'twenty', 'them', 'without', 'neither', 'still', 'amount', 'myself', \"'ve\", 'from', 'meanwhile', 'again', 'fifty', 'even', 'after', 'therein', 'four', 'which', 'hereupon', 'eleven', 'always', 'will', 'one', 'someone', 'behind', 'must', 'others', 'within', 'than', 'against', 'only', 'for', 'itself', '’m', 'unless', 'why', 'her', 'up', 'everywhere', 'whether', '’ve', 'becoming', 'his', 'are', 'should', 'back', 'go'}\n"
     ]
    }
   ],
   "source": [
    "#adding 5 stop words to the default list\n",
    "nlp.Defaults.stop_words |= {'Word1','Word2','Word3','Word4','Word5'}         \n",
    "print(nlp.Defaults.stop_words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9fc2a288",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'but', \"'s\", '‘ll', 'nor', 'often', \"n't\", 'through', 'ourselves', 'upon', 'everyone', 'us', 'themselves', 'say', 'whither', 'whereafter', 'former', 'n‘t', 'nothing', 'have', 'give', 'whoever', 'has', 'under', 'except', \"'d\", 'regarding', 'whose', 'hereby', 'full', 'least', 'take', 'beyond', 'thru', 'made', 'at', 'everything', 'became', 'whereupon', 'move', 'did', 'whatever', 'else', 'own', 'any', 'used', '‘re', 'although', 'formerly', 'wherein', 'next', 'name', 'down', 'five', 'out', 'using', 'would', 'who', \"'re\", 'elsewhere', 'before', 'last', 'that', 'top', 'twelve', 'until', 'seems', 'eight', 'alone', 'together', 'not', 'therefore', 'now', 'since', 'otherwise', 'less', 'put', 'all', 'seemed', 'n’t', 'your', 'most', 'they', 'amongst', 'few', 'Word5', 'perhaps', 'off', 'she', 'whereas', 'noone', 'hers', 'anyone', 'such', 'yourself', 'really', 'get', 'mine', 'part', 'many', \"'m\", 'well', 'our', 're', 'side', 'several', 'six', 'Word1', 'had', 'yourselves', 'ours', '‘m', 'being', 'yours', 'empty', 'what', 'among', 'the', 'done', 'nine', 'doing', 'toward', 'first', 'further', 'was', 'enough', 'much', 'thereby', 'latter', 'another', 'seeming', 'Word2', 'almost', 'be', 'into', 'please', 'serious', 'thereafter', 'become', 'below', 'this', 'hence', 'might', 'bottom', '‘d', 'just', 'both', '‘s', 'anything', 'sixty', 'to', 'cannot', 'too', \"'ll\", 'besides', 'somehow', 'we', 'each', 'by', 'or', 'ten', 'do', 'so', 'forty', 'sometime', 'towards', 'my', 'three', 'same', 'see', 'because', 'front', 'across', 'fifteen', 'call', 'herein', 'if', 'sometimes', 'nowhere', 'wherever', 'thus', '’re', 'third', 'indeed', 'seem', 'thereupon', 'whom', 'ca', 'in', 'throughout', 'other', '‘ve', 'over', 'is', 'ever', 'while', 'himself', 'anyhow', 'then', 'were', 'per', 'when', 'however', 'here', 'of', 'their', 'some', 'does', 'with', 'latterly', 'also', 'he', 'no', 'various', 'mostly', 'make', 'moreover', '’ll', 'quite', 'been', 'keep', 'above', 'about', 'on', 'nevertheless', 'whole', 'namely', 'there', 'whenever', 'an', 'herself', 'once', 'something', 'anywhere', 'rather', 'him', 'whence', 'a', 'though', 'during', 'along', 'hundred', 'its', 'may', 'you', 'onto', 'either', 'very', 'hereafter', 'those', 'none', 'via', '’d', 'and', 'beside', 'how', 'yet', 'where', 'it', 'am', 'nobody', '’s', 'can', 'around', 'as', 'Word3', 'i', 'anyway', 'Word4', 'beforehand', 'whereby', 'already', 'every', 'show', 'me', 'could', 'thence', 'afterwards', 'these', 'due', 'somewhere', 'two', 'more', 'twenty', 'them', 'without', 'neither', 'still', 'amount', 'myself', \"'ve\", 'from', 'meanwhile', 'again', 'fifty', 'even', 'after', 'therein', 'four', 'which', 'hereupon', 'eleven', 'will', 'one', 'someone', 'behind', 'must', 'others', 'within', 'than', 'against', 'only', 'for', 'itself', '’m', 'unless', 'why', 'her', 'up', 'everywhere', 'whether', '’ve', 'becoming', 'his', 'are', 'should', 'back', 'go'}\n"
     ]
    }
   ],
   "source": [
    "#removing always,never,between,becomes stop words\n",
    "nlp.Defaults.stop_words -= {'always','never','between','becomes'}          \n",
    "print(nlp.Defaults.stop_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387a88c3",
   "metadata": {},
   "source": [
    "### TASK 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b813bf84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA HIGH SCHOOL - DAY\n",
      "Revision November 12, 1997\n",
      "I hope dinner's ready because I only have ten minutes before Mrs. Johnson squirts out a screamer.\n",
      "He grabs the mail and rifles through it, as he bends down to kiss Sharon on the cheek.\n",
      "MICHAEL- C'mon. I'm supposed to give you the tour. They head out of the office\n",
      "MICHAEL (continuing)- So -- which Dakota you from?\n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#opening the file and reading the data\n",
    "raw=open('Raw_data_for_analysis.txt').read()\n",
    "Raw=nlp(raw)\n",
    "print(Raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9bcc2156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PADUA', 'HIGH', 'SCHOOL', '-', 'DAY', '\\n', 'Revision', 'November', '12', ',', '1997', '\\n', 'I', 'hope', 'dinner', \"'s\", 'ready', 'because', 'I', 'only', 'have', 'ten', 'minutes', 'before', 'Mrs.', 'Johnson', 'squirts', 'out', 'a', 'screamer', '.', '\\n', 'He', 'grabs', 'the', 'mail', 'and', 'rifles', 'through', 'it', ',', 'as', 'he', 'bends', 'down', 'to', 'kiss', 'Sharon', 'on', 'the', 'cheek', '.', '\\n', 'MICHAEL-', \"C'm\", 'on', '.', 'I', \"'m\", 'supposed', 'to', 'give', 'you', 'the', 'tour', '.', 'They', 'head', 'out', 'of', 'the', 'office', '\\n', 'MICHAEL', '(', 'continuing)-', 'So', '--', 'which', 'Dakota', 'you', 'from', '?', '\\n          \\n                                 \\n']\n",
      "\n",
      "No.of tokens= 46\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZATION\n",
    "print ([token.text for token in Raw]) \n",
    "print('\\nNo.of tokens=',len(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dea12f95",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence after removal of stop words:\n",
      " [PADUA, HIGH, SCHOOL, -, DAY, \n",
      ", Revision, November, 12, ,, 1997, \n",
      ", hope, dinner, ready, minutes, Mrs., Johnson, squirts, screamer, ., \n",
      ", grabs, mail, rifles, ,, bends, kiss, Sharon, cheek, ., \n",
      ", MICHAEL-, C'm, ., supposed, tour, ., head, office, \n",
      ", MICHAEL, (, continuing)-, --, Dakota, ?, \n",
      "          \n",
      "                                 \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#Remove stop words from file\n",
    "filtered_data2=[]                                                                                                                     \n",
    "for word in Raw:                                                         \n",
    "    if word.is_stop==False:                                               \n",
    "        filtered_data2.append(word)                                        \n",
    "print(\"Sentence after removal of stop words:\\n\",filtered_data2)                                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2514dbc0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA ->  PADUA\n",
      "HIGH ->  HIGH\n",
      "SCHOOL ->  SCHOOL\n",
      "- ->  -\n",
      "DAY ->  DAY\n",
      "\n",
      " ->  \n",
      "\n",
      "Revision ->  Revision\n",
      "November ->  November\n",
      "12 ->  12\n",
      ", ->  ,\n",
      "1997 ->  1997\n",
      "\n",
      " ->  \n",
      "\n",
      "hope ->  hope\n",
      "dinner ->  dinner\n",
      "ready ->  ready\n",
      "minutes ->  minute\n",
      "Mrs. ->  Mrs.\n",
      "Johnson ->  Johnson\n",
      "squirts ->  squirt\n",
      "screamer ->  screamer\n",
      ". ->  .\n",
      "\n",
      " ->  \n",
      "\n",
      "grabs ->  grab\n",
      "mail ->  mail\n",
      "rifles ->  rifle\n",
      ", ->  ,\n",
      "bends ->  bend\n",
      "kiss ->  kiss\n",
      "Sharon ->  Sharon\n",
      "cheek ->  cheek\n",
      ". ->  .\n",
      "\n",
      " ->  \n",
      "\n",
      "MICHAEL- ->  MICHAEL-\n",
      "C'm ->  come\n",
      ". ->  .\n",
      "supposed ->  suppose\n",
      "tour ->  tour\n",
      ". ->  .\n",
      "head ->  head\n",
      "office ->  office\n",
      "\n",
      " ->  \n",
      "\n",
      "MICHAEL ->  MICHAEL\n",
      "( ->  (\n",
      "continuing)- ->  continuing)-\n",
      "-- ->  --\n",
      "Dakota ->  Dakota\n",
      "? ->  ?\n",
      "\n",
      "          \n",
      "                                 \n",
      " ->  \n",
      "          \n",
      "                                 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lemmatization after removal of stop words      \n",
    "for word in filtered_data2:                                                      \n",
    "    print(word.text,'-> ',word.lemma_)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0c2685d9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PADUA \t PROPN \t NNP\n",
      "HIGH \t PROPN \t NNP\n",
      "SCHOOL \t PROPN \t NNP\n",
      "- \t PUNCT \t HYPH\n",
      "DAY \t PROPN \t NNP\n",
      "\n",
      " \t SPACE \t _SP\n",
      "Revision \t PROPN \t NNP\n",
      "November \t PROPN \t NNP\n",
      "12 \t NUM \t CD\n",
      ", \t PUNCT \t ,\n",
      "1997 \t NUM \t CD\n",
      "\n",
      " \t SPACE \t _SP\n",
      "hope \t VERB \t VBP\n",
      "dinner \t NOUN \t NN\n",
      "ready \t ADJ \t JJ\n",
      "minutes \t NOUN \t NNS\n",
      "Mrs. \t PROPN \t NNP\n",
      "Johnson \t PROPN \t NNP\n",
      "squirts \t VERB \t VBZ\n",
      "screamer \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "grabs \t VERB \t VBZ\n",
      "mail \t NOUN \t NN\n",
      "rifles \t NOUN \t NNS\n",
      ", \t PUNCT \t ,\n",
      "bends \t VERB \t VBZ\n",
      "kiss \t VERB \t VB\n",
      "Sharon \t PROPN \t NNP\n",
      "cheek \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL- \t PROPN \t NNP\n",
      "C'm \t VERB \t VBZ\n",
      ". \t PUNCT \t .\n",
      "supposed \t VERB \t VBN\n",
      "tour \t NOUN \t NN\n",
      ". \t PUNCT \t .\n",
      "head \t VERB \t VBP\n",
      "office \t NOUN \t NN\n",
      "\n",
      " \t SPACE \t _SP\n",
      "MICHAEL \t PROPN \t NNP\n",
      "( \t PUNCT \t -LRB-\n",
      "continuing)- \t NOUN \t NNS\n",
      "-- \t PUNCT \t :\n",
      "Dakota \t PROPN \t NNP\n",
      "? \t PUNCT \t .\n",
      "\n",
      "          \n",
      "                                 \n",
      " \t SPACE \t _SP\n"
     ]
    }
   ],
   "source": [
    "#POS tagging after removal of stop words.\n",
    "for word in filtered_data2:\n",
    "    print(word.text,'\\t',word.pos_,'\\t',word.tag_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
